{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "\n",
    "from cmath import nan\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pyparsing import col # For plotting data\n",
    "import seaborn as sns # For plotting data\n",
    "\n",
    "# Selection of ml algorithms for prediction\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "#import packages for text preprocessing\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "\n",
    "# For setting up pipeline\n",
    "from sklearn.pipeline import Pipeline \n",
    "\n",
    "# Various pre-processing steps\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#hyperparametertuning\n",
    "from optuna.integration import OptunaSearchCV\n",
    "from optuna.distributions import *\n",
    "\n",
    "#Feature Selection\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "#Target Transformation\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "#Metrics\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "#Safe Model\n",
    "from joblib import dump, load\n",
    "\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Process Types and T-Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for importing dataframes\n",
    "ROOT_DIR = os.path.abspath(os.curdir)\n",
    "ROOT_DIR = ROOT_DIR.replace(os.sep, '/')\n",
    "ROOT_DIR = str(Path(ROOT_DIR).parent.parent)\n",
    "print(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define t-times\n",
    "#needs to be adapted according to the number of gates considered \n",
    "TIMES = range(0,4) #(Example: TIMES = range(0,4) corresponds to a KIP with its beginning t=0 and three gates)\n",
    "\n",
    "#define the targets for every t\n",
    "#the total lead time should be calculated before and added as attribute at the trace-level of the event log\n",
    "targets_T0 = [\"trace:total_leadtime\"]\n",
    "targets_T1 = [\"trace:total_leadtime\"] \n",
    "targets_T2 = [\"trace:total_leadtime\"]\n",
    "targets_T3 = [\"trace:total_leadtime\"]\n",
    "\n",
    "targets = [targets_T0, targets_T1, targets_T2,  targets_T3]\n",
    "\n",
    "t_with_score = pd.DataFrame(columns=[\"Time\", \"MAE\", \"Model\"])\n",
    "row = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Text Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load and extract textual features from XML\n",
    "#put your event_log with the textual information into the Input Folder\n",
    "with open(ROOT_DIR + \"/Input Data/event_log.xml\", 'r', encoding='utf-8') as f:\n",
    "    logs = f.read()\n",
    "logs_data = BeautifulSoup(logs, \"xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts_for_t(tn):\n",
    "  texts = \"\"\n",
    "    \n",
    "  docs = tn.find_all(\"doc\")\n",
    "  for d in docs:\n",
    "    remarks = d.find(\"remarks\")\n",
    "    if remarks:\n",
    "      texts = texts + \"; \" + remarks.text\n",
    "\n",
    "  parts = tn.find_all(\"part\")\n",
    "  for p in parts:\n",
    "    descde = p.find(\"descde\")\n",
    "    if descde:\n",
    "      texts = texts + \"; \" + descde.text\n",
    "\n",
    "  task_events = tn.find_all(\"task_event\")\n",
    "  for t in task_events:\n",
    "    if (t.find(\"event_type\").text == \"Aufgabe Ereignis\"):\n",
    "      description = t.find(\"description\")\n",
    "      if description:\n",
    "        texts = texts + \"; \" + description.text\n",
    "  \n",
    "  return(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "cases = logs_data.find_all(\"case\")\n",
    "for c in cases:\n",
    "  entry = {}\n",
    "\n",
    "  entry[\"trace:cdb_ec_id\"] = c.attrs[\"case_id\"]\n",
    "  header = \"\" \\\n",
    "    + c.find(\"name\").text + \"; \" \\\n",
    "    + c.find(\"description\").text + \"; \"  \\\n",
    "    + c.find(\"notice\").text + \"; \"  \\\n",
    "    + c.find(\"compatibility\").text + \"; \"  \\\n",
    "    + c.find(\"distribution\").text + \"; \"  \\\n",
    "    + c.find(\"reason\").text\n",
    "  \n",
    "  t0 = c.find(\"time\", {\"T\" : \"0\"})\n",
    "  t1 = c.find(\"time\", {\"T\" : \"1\"})\n",
    "  t2 = c.find(\"time\", {\"T\" : \"2\"})\n",
    "  t3 = c.find(\"time\", {\"T\" : \"3\"})\n",
    "  entry[\"t0\"] = header + get_texts_for_t(t0)\n",
    "  entry[\"t1\"] = header + get_texts_for_t(t0) + get_texts_for_t(t1)\n",
    "  entry[\"t2\"] = header + get_texts_for_t(t0) + get_texts_for_t(t1) + get_texts_for_t(t2)\n",
    "  entry[\"t3\"] = header + get_texts_for_t(t0) + get_texts_for_t(t1) + get_texts_for_t(t2) + get_texts_for_t(t3)\n",
    "  \n",
    "  texts.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = pd.DataFrame(texts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stemmer = SnowballStemmer(\"german\")\n",
    "stop_words = set(stopwords.words(\"german\"))\n",
    "\n",
    "def clean_text(text, for_embedding=False):\n",
    "    \"\"\"\n",
    "        - remove any html tags (< /br> often found)\n",
    "        - Keep only ASCII + European Chars and whitespace, no digits\n",
    "        - remove single letter chars\n",
    "        - convert all whitespaces (tabs etc.) to single wspace\n",
    "        if not for embedding (but e.g. tdf-idf):\n",
    "        - all lowercase\n",
    "        - remove stopwords, punctuation and stemm\n",
    "    \"\"\"\n",
    "    RE_WSPACE = re.compile(r\"\\s+\", re.IGNORECASE)\n",
    "    RE_TAGS = re.compile(r\"<[^>]+>\")\n",
    "    RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž ]\", re.IGNORECASE)\n",
    "    RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž]\\b\", re.IGNORECASE)\n",
    "    if for_embedding:\n",
    "        # Keep punctuation\n",
    "        RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž,.!? ]\", re.IGNORECASE)\n",
    "        RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž,.!?]\\b\", re.IGNORECASE)\n",
    "\n",
    "    text = re.sub(RE_TAGS, \" \", str(text))\n",
    "    text = re.sub(RE_ASCII, \" \", str(text))\n",
    "    text = re.sub(RE_SINGLECHAR, \" \", str(text))\n",
    "    text = re.sub(RE_WSPACE, \" \", str(text))\n",
    "\n",
    "    word_tokens = word_tokenize(text)\n",
    "    words_tokens_lower = [word.lower() for word in word_tokens]\n",
    "\n",
    "    if for_embedding:\n",
    "        # no stemming, lowering and punctuation / stop words removal\n",
    "        words_filtered = word_tokens\n",
    "    else:\n",
    "        words_filtered = [\n",
    "            stemmer.stem(word) for word in words_tokens_lower if word not in stop_words\n",
    "        ]\n",
    "\n",
    "    text_clean = \" \".join(words_filtered)\n",
    "    return text_clean"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for TIME in TIMES:\n",
    "    print(TIME)\n",
    "    \n",
    "    #LOAD TRAIN AND TEST DATASET\n",
    "    df_train_raw = pd.read_csv(ROOT_DIR + \"/Input Data/Train_Features_t\" + str(TIME) + \".csv\")\n",
    "    df_test_raw = pd.read_csv(ROOT_DIR + \"/Input Data/Test_Features_t\" + str(TIME) + \".csv\")\n",
    "\n",
    "    #MERGE TRAIN AND TEST DATASET WITH TEXTUAL INFORMATION based on the case ID\n",
    "    df_train_raw = pd.merge(left=df_train_raw, right=texts[[\"t\" + str(TIME), \"trace:case_id\"]], left_on='trace:case_id', right_on='trace:case_id', how='left')\n",
    "    df_test_raw = pd.merge(left=df_test_raw, right=texts[[\"t\" + str(TIME), \"trace:case_id\"]], left_on='trace:case_id', right_on='trace:case_id', how='left')\n",
    "\n",
    "    df_train_raw.drop('trace:concept:name', inplace = True, axis=1)\n",
    "    df_test_raw.drop('trace:concept:name', inplace = True, axis=1)\n",
    "\n",
    "    #DEFINE TARGETS\n",
    "    target = targets[TIME][0]\n",
    "    to_drop = targets[TIME]\n",
    "    to_drop.append(\"trace:case_id\")\n",
    "\n",
    "    #DEFINE DATAFRAME FOR TRAIN SET\n",
    "    df_train = df_train_raw.copy()\n",
    "    df_train = df_train.dropna()\n",
    "    y_train = df_train[target].to_frame() \n",
    "    train_ids = df_train[\"trace:case_id\"]\n",
    "    X_train = df_train.drop(to_drop, axis=1)\n",
    "\n",
    "    #DEFINE DATAFRAME FOR TEST SET\n",
    "    df_test = df_test_raw.copy()\n",
    "    df_test = df_test.dropna()\n",
    "    y_test = df_test[target].to_frame()\n",
    "    test_ids = df_test[\"trace:case_id\"]\n",
    "    X_test = df_test.drop(to_drop, axis=1)\n",
    "\n",
    "    #clean text\n",
    "    X_train[\"t\"+str(TIME)] = X_train.loc[X_train[\"t\"+str(TIME)].str.len() > 20, \"t\"+str(TIME)]\n",
    "    X_train[\"t\"+str(TIME)] = X_train[\"t\"+str(TIME)].map(\n",
    "    lambda x: clean_text(x, for_embedding=False) if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "    X_test[\"t\"+str(TIME)] = X_test.loc[X_test[\"t\"+str(TIME)].str.len() > 20, \"t\"+str(TIME)]\n",
    "    X_test[\"t\"+str(TIME)] = X_test[\"t\"+str(TIME)].map(\n",
    "    lambda x: clean_text(x, for_embedding=False) if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "    #ENCODE CATEGORICAL VARIABLES\n",
    "    categoric_datatypes = ['bool', 'object']\n",
    "\n",
    "    numeric_train = X_train.select_dtypes(exclude=categoric_datatypes)\n",
    "    categoric_train = X_train.select_dtypes(include=categoric_datatypes)\n",
    "    categoric_train = categoric_train.drop('t'+str(TIME), axis=1)\n",
    "    numeric_columns_train = numeric_train.columns.to_list()\n",
    "    categoric_columns_train = categoric_train.columns.to_list() \n",
    "    text_columns_train = list(['t'+str(TIME)])\n",
    "\n",
    "    numeric_test = X_test.select_dtypes(exclude=categoric_datatypes)\n",
    "    categoric_test = X_test.select_dtypes(include=categoric_datatypes)\n",
    "    categoric_test = categoric_test.drop('t'+str(TIME), axis=1)\n",
    "    numeric_columns_test = numeric_test.columns.to_list()\n",
    "    categoric_columns_test = categoric_test.columns.to_list()\n",
    "    text_columns_test = list(['t'+str(TIME)])\n",
    "\n",
    "    s1 = set(numeric_columns_train)\n",
    "    s2 = set(numeric_columns_test)\n",
    "\n",
    "    s_intersect = list(s1 & s2)\n",
    "    s_intersect = s_intersect + categoric_columns_test + text_columns_test\n",
    "    \n",
    "    X_test = X_test[s_intersect]\n",
    "    X_train = X_train[s_intersect]\n",
    "\n",
    "    numeric_train_new = X_train.select_dtypes(exclude=categoric_datatypes)\n",
    "    numeric_columns_train_new = numeric_train_new.columns.to_list()\n",
    "\n",
    "    #DEFINE REGRESSION MODELS\n",
    "    regressionModels = [\n",
    "        RandomForestRegressor(random_state=0),\n",
    "        Ridge(random_state=0),\n",
    "        GradientBoostingRegressor(random_state=0),\n",
    "        MLPRegressor(random_state=0, max_iter=10000, learning_rate='adaptive', early_stopping=True, tol=float(2), batch_size=32),\n",
    "        DummyRegressor(),\n",
    "        ]\n",
    "\n",
    "    print(TIME)\n",
    "\n",
    "    #DEFINE TOPIC MODELING\n",
    "\n",
    "    topic_modeling = Pipeline([\n",
    "        (\"vectorizer\", CountVectorizer(max_df=0.7, min_df=0.2)),\n",
    "        (\"TopicModeling\", LatentDirichletAllocation(n_components=10, random_state=0)),\n",
    "    ])\n",
    "\n",
    "    #DEFINE COLUMN TRANSFORMER\n",
    "    ct = ColumnTransformer([\n",
    "        (\"OneHot\", OneHotEncoder(handle_unknown='ignore'), categoric_columns_train),\n",
    "        (\"scale\", MinMaxScaler(), numeric_columns_train_new),\n",
    "        (\"topic_modeling\", topic_modeling, \"t\"+str(TIME)),\n",
    "        ])\n",
    "    \n",
    "    #HYPERPARAMETERTUNING WITH OPTUNA SEARCH\n",
    "    #define dictionaries for each algorithm\n",
    "    RandomForestDict = {'regression__regressor__n_features_to_select': FloatDistribution(low=0.2, high=0.7),'regression__regressor__estimator__n_estimators': IntDistribution(low=100, high=2000, step=1), 'regression__regressor__estimator__max_depth': IntDistribution(low=10, high=20, step=1), 'regression__regressor__estimator__max_features': FloatDistribution(low=0.1, high=0.7)} #spannen angeben\n",
    "    RidgeDict = {'regression__regressor__n_features_to_select': FloatDistribution(low=0.2, high=0.7),'regression__regressor__estimator__alpha': FloatDistribution(low=0, high=10)}\n",
    "    GradientBoostingDict = {'regression__regressor__n_features_to_select': FloatDistribution(low=0.2, high=0.7),'regression__regressor__estimator__n_estimators': IntDistribution(low=100, high=2000, step=1), 'regression__regressor__estimator__learning_rate': FloatDistribution(low=1e-5, high=0.2), 'regression__regressor__estimator__max_depth': IntDistribution(low=10, high=14, step=1)}\n",
    "    MLPDict = {'regression__regressor__hidden_layer_sizes': IntDistribution(low=20, high=512),'regression__regressor__learning_rate_init': FloatDistribution(low=1e-5, high=0.2), 'regression__regressor__max_iter': IntDistribution(low=5, high=250)}\n",
    "    Dummy = {'regression__regressor__strategy': CategoricalDistribution(['mean'])}\n",
    "\n",
    "    #dictionaries werden als Liste mit in die Pipeline gegeben, damit sich für jeden algorithmus auf das richtige dict bezogen werden kann\n",
    "    param_list = [\n",
    "        RandomForestDict,\n",
    "        RidgeDict,\n",
    "        GradientBoostingDict,\n",
    "        MLPDict,\n",
    "        Dummy,\n",
    "        ]\n",
    "    \n",
    "    name_list = [\n",
    "        \"RandomForest\",\n",
    "        \"Ridge\",\n",
    "        \"GradientBoosting\",\n",
    "        \"MLP\",\n",
    "        \"Dummy\",\n",
    "        ]\n",
    "\n",
    "    #DEFINE PIPELINE    \n",
    "    for regressor, param_dict, name in zip(regressionModels,param_list,name_list):\n",
    "        if (name == 'RandomForest') or (name == \"Ridge\") or (name == \"GradientBoosting\"):\n",
    "            rfe = RFE(estimator=regressor, step=10)\n",
    "            y_transformer = TransformedTargetRegressor(regressor=rfe, func=np.log1p, inverse_func=np.expm1)\n",
    "        else:\n",
    "            y_transformer = TransformedTargetRegressor(regressor=regressor, func=np.log1p, inverse_func=np.expm1)\n",
    "        pipe = Pipeline(steps=[\n",
    "            ('encoding_scaling', ct),\n",
    "            ('regression', y_transformer)\n",
    "            ])\n",
    "        \n",
    "        #PERFORM OPTUNA SEARCH\n",
    "        gs = OptunaSearchCV(estimator=pipe, param_distributions=param_dict, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1, n_trials=15, random_state=42)\n",
    "        \n",
    "        gs.fit(X_train, y_train.values.ravel()) #Perform fit on Optuna Search instead of Pipeline\n",
    "        model = gs.best_estimator_ #The best estimator is taken and fitted\n",
    "        dump(model, ROOT_DIR + \"/Prediction Models/\"+name+\"_\"+str(TIME)+\"_withTopicModeling.joblib\") \n",
    "        predictions = model.predict(X_test)\n",
    "        score_mae = mean_absolute_error(y_test, predictions)\n",
    "\n",
    "        pd.DataFrame(columns=[\"Time\", \"MAE\", \"Model\"])\n",
    "        t_with_score.loc[row] = [TIME, score_mae, str(regressor)]\n",
    "        row = row + 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results and Vizualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_with_score.to_csv(ROOT_DIR + \"/Results/PredictionMetrics_withTopicModeling.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = sns.relplot(x=\"Time\", y=\"MAE\", hue=\"Model\", kind=\"line\", data=t_with_score)\n",
    "pl.set(ylim=(0, 200))\n",
    "pl.set(xticks=list(TIMES))\n",
    "plt.title(\"MAE for Predictions with Topic Modeling\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scikit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4bcda084bfad823754408a7109bf63e95aec8bd82837b5a0078ca8bb13f534a1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
